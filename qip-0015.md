```
  QIP: 15
  Layer: Consensus (hard fork) 
  Title: Qi UTXO Set Trimming
  Author: jdowning100
  Comments-Summary: No comments yet.
  Comments-URI: https://github.com/quai-network/qips/wiki/Comments:QIP-00015
  Status: Active
  Type: Standards Track
  Created: 2024-09-06
  License: BSD-2-Clause
  Replaces: QIP-0012
```

## Abstract

The UTXO set is the set of all unspent transaction outputs in the Qi ledger. Qi is a money system with cash-like properties, such as denominations that can be broken down into smaller denominations. Combining smaller denominations into larger ones can also be done with a different mechanism, but that is to be discussed in a separate QIP. This QIP proposes a mechanism to regularly remove dust UTXOs from the UTXO set.

## Motivation

The Qi UTXO set must be held on the disk of every node in the network in order for nodes to come into consensus, and the set can grow infinitely large. There are currently about [187.3 million UTXOs](https://www.blockchain.com/explorer/charts/utxo-count) in Bitcoin, and at ~70ish bytes per UTXO, takes up about 13GB of space on disk. 
To build a monetary system for every individual the UTXO set could grow to as large as 20 billion for a given Zone, which is about 200 UTXOs per user for 100 million users per Zone, for example. A Zone that processes 1,000 transactions per second would allow 100  million users to send one transaction per day, and if the demand grows larger, a new Zone would be added. Qi UTXOs are about 57 bytes: 2 bytes for the leveldb prefix, 32 bytes for the txhash, 2 bytes for the index, 20 bytes for the address that may spend the UTXO, 1 byte for the denomination, and 1 byte or more for the lock (if applicable). This means that the UTXO set on disk could grow to as large as 1.14 TB per Zone. 
Because the UTXO set can grow to be infinitely large, a well-funded attacker could launch a DDoS attack to grow the UTXO set to a size that is untenable for the average node to store on disk. In order to prevent this attack, this QIP proposes removing old, unused small-value UTXOs (below 1 Qi), such that the DDoS bloating attack would be expontentially more expensive to carry out - a bloating attack to increase the size of the UTXO set by 10 billion UTXOs would cost 10 billion Qi vs 10 million Qi, for example.  

## Specification

### UTXO Trimming

The following table displays small-value denominations and the timeframe after which each respective UTXO should be trimmed (removed from the set). Note that denomination 0 is equivalent to 0.001 Qi or 1 "Qit" (the smallest unit of Qi).

| Denomination      | Value                 | Timeframe                         
|-------------------|-----------------------|-------------------
| 0                 | 0.001 Qi (1 "Qit")    | 2 weeks                      
| 1                 | 0.005 Qi (5 "Qits")   | 1 month                   
| 2                 | 0.01 Qi (10 "Qits")   | 2 months 
| 3                 | 0.05 Qi (50 "Qits")   | 4 months          
| 4                 | 0.1 Qi (100 "Qits")   | 8 months         
| 5                 | 0.25 Qi               | 10 months         
| 6                 | 0.5 Qi                | 12 months 

Every block will trim UTXOs created by a block at a depth that is determined to be approximately equal to each timeframe. During block processing, the node must remove UTXOs of a certain denomination created by 7 different blocks from the set and ensure that the MuHash of the set after the trimming is equal to the MuHash in the header. This can be compute intensive, so it's recommended that the implementation perform the trimming of each block in parallel, which requires a CPU with 7 threads at minimum.
The trimming must happen synchronously as part of the block append process, as the result of the trimming process modifies the UTXO set commitment and must be followed by every node (but not necessarily in a specific order, which is a property unique to the MuHash commitment).
An attacker that wishes to bloat the size of the set with the smallest denomination would be able to do so only within 2 weeks. If a Zone can support a throughput of 1000 UTXOs per second (this is simply an estimate of 500 transactions per second that create 2 UTXOs per transaction) the attacker would be able to add 1.2 billion UTXOs to the set before the UTXOs start being deleted. This is assuming that the attacker is able to fill every block with only his transactions, which becomes expontentially more expensive as market participants start competing for inclusion, but is still a reasonable size for all nodes to continue operating (an additional ~68 GB).

### UTXO Key Organization
The UTXO set is a flat database keyed by transaction hash and index, not by block number or block hash. Because we must trim from blocks at depth, we must store the UTXOs created by each block on disk. One way to do this is to add the block number and denomination into the key as a prefix, and then iterate through all keys on disk with the block number and denomination as the prefix and trim the UTXOs under that prefix.
The issue with this approach is that it requires the user or sender of a transaction to provide the block number and denomination into the input so the UTXO can be looked up for verification. Providing the denomination is easy, as the user knows how much they will spend, but requiring block number is much more difficult, as the block height that created a UTXO can change at the tip. A wallet might mistakenly mark that a UTXO was created in a certain block, only for that information to be invalidated 5 minutes later. Once this happens, if the wallet is offline, the UTXO becomes very difficult to find without running a node. Additionally, this approach makes high-frequency applications or micropayment platforms that immediately spend UTXOs much more difficult to build, and increases the size of every input by 9 bytes - 8 bytes for the block height and 1 byte for the denomination, which increases storage requirements.

A different approach is to store the aforementioned 9 bytes in a different way - store the key of every UTXO created by a block on disk, but shorten the key to 9 bytes: the first 8 bytes of the txhash and the 1-byte denomination. This is a separate index that maps block height to shortened UTXO keys. The storage requirements are similar to the prior approach, but the user does not have to provide extra information that takes up bandwidth in transaction propagation. The UTXO "index" field is not necessary for the keys, as all indices of a transaction for a specific block and denomination will be trimmed. In addition, only keys for UTXOs of denominations 0 through 6 need to be stored, as all other UTXOs will never be trimmed.

The birthday problem formula provides an estimate of the entropy provided by an 8-byte hash. For 5 billion random elements, the chance of one unintentional collision is 50%. However, a malicious actor could intentionally create collisions. To account for this, the proposed rule is simple: if there is a duplicate key, and the denomination is the same, delete both UTXOs. While this is incredibly unlikely, it would be very difficult to determine which UTXO was created in the block we are trying to trim without having the full historical block data, which most nodes will prune out.
Even in such an unlikely case, the value that is trimmed will be below 1 Qi, so it is a reasonable approach to take to reduce data storage across all nodes in the network. If a malicious actor intentionally creates more duplicate keys than can reasonably be trimmed in a single block, the duplicate key is stored to disk, and the next block will continue the trimming process to remove the rest of the duplicates. The current limit for the number of duplicate keys is 1000 per key per block.
The block height to UTXO mapping could be ordered by denomination in the key, so that when a block is to be trimmed, the process can efficiently find all denominations that are to be trimmed without searching through all UTXOs created in the block.

### Trim Depths Relative to Set Size
Each denomination has a different trim depth. That is, the block depth at which a UTXO is trimmed depends on its denomination. These trim depths are not fixed, rather, they vary with respect to the UTXO set size. To address a potential bloating attack, as the UTXO set size grows larger, the trimming for each respective denomination happens earlier, and thus the trim depth shortens. I suggest four levels, and each level corresponds to a shortening of each respective trim depth relative to the UTXO set size.

| UTXO Set Size     | Trim Depth Reduction                       
|-------------------|-----------------------
| 0 -> 10 billion   | None                        
| 10 billion+       | Decrease lifespan by 10%                      
| 15 billion+       | Decrease lifespan by an additional 20%   
| 18 billion+       | Decrease lifespan by an additional 50%          
| 20 billion+       | Decrease lifespan by an additional 50%       
            
This creates a 'soft' limit of the UTXO set size of 20 billion UTXOs. This limit is 'soft' because the UTXO ledger does not stop functioning after the limit has been hit, but UTXOs are trimmed at a very high rate which will make a bloating attack incredibly expensive. If the UTXO set has grown to 20 billion naturally due to user adoption, the uncle rate will likely have also grown to a rate that is unsustainable and will cause a network expansion. The added friction created by regular UTXO trimming will incentivize users to move to the new zone, causing a gradual reduction in the set size. Note that when the set size shrinks below a threshold defined above, the trim depths will return to their previous levels.


### Snap Syncing
One interesting outcome of this QIP is that the network must come to consensus on which UTXOs were created in blocks up to a year prior to the tip. This is easy for nodes that keep historical data, or nodes that have synced every block from genesis, but more difficult for nodes that wish to snap sync to a certain height (perhaps 1 month behind the tip) as they will not have the data necessary to construct the UTXO set with proper block height mappings.
To account for this, a snap-synced node must download all headers and Qi transactions from the max trim depth to the tip and construct either a mapping of block height to UTXO keys or a UTXO set prefixed by the block height. This would extend the time necessary to complete a snap sync, but its unclear exactly how long it would take. To make things faster, the header could be modified to include a MuHash commitment of the 9-byte UTXO keys created in that block, so only the header and list of 9-byte UTXO keys need to be downloaded for each block which might reduce download requirements.
Another approach is to have a separate MuHash commitment to the entire block height to UTXO index that is updated on each block. This way, the entire index could be downloaded at once and compared to the header commitment for verification.

## Copyright
This QIP licensed under the BSD 2-clause license.
